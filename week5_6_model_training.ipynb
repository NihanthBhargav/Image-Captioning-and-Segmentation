{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635cb732",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c43289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bollejayanthsriteja\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and metadata...\n",
      "Vocab size: 8832, Max length: 38\n",
      "Number of images for training: 8092\n",
      "Defining model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ seq_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,260,992</span> │ seq_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ seq_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8832</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,269,824</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ seq_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m2,260,992\u001b[0m │ seq_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ seq_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m525,312\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8832\u001b[0m)      │  \u001b[38;5;34m2,269,824\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,646,464</span> (21.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,646,464\u001b[0m (21.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,646,464</span> (21.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,646,464\u001b[0m (21.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch (generator length): 506\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bollejayanthsriteja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2097 - loss: 5.0786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bollejayanthsriteja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:276: UserWarning: Can save best model only with val_loss available.\n",
      "  if self._should_save_model(epoch, batch, logs, filepath):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1223s\u001b[0m 2s/step - accuracy: 0.2715 - loss: 4.3029\n",
      "Epoch 2/5\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m900s\u001b[0m 2s/step - accuracy: 0.3553 - loss: 3.3226\n",
      "Epoch 3/5\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m857s\u001b[0m 2s/step - accuracy: 0.3782 - loss: 3.0309\n",
      "Epoch 4/5\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1816s\u001b[0m 4s/step - accuracy: 0.3938 - loss: 2.8484\n",
      "Epoch 5/5\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m860s\u001b[0m 2s/step - accuracy: 0.4051 - loss: 2.7092\n",
      "Model saved to ./models\\caption_model_final.keras\n",
      "Loading best model for evaluation: ./models\\caption_model_final.keras\n",
      "BLEU (avg) on sample: 0.1401\n",
      "Test image: 1000268201_693b08cb0e.jpg\n",
      "Generated caption: <start> a child is sitting on a bench <end>\n"
     ]
    }
   ],
   "source": [
    "# Week 5-6: Image Captioning - Model Training & Evaluation\n",
    "# Notebook-friendly (no argparse). Set paths below.\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#  CONFIG - set your paths \n",
    "images_dir = \"./images\"\n",
    "features_dir = \"./features\"\n",
    "processed_dir = \"./processed\"\n",
    "models_dir = \"./models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Quick-run options:\n",
    "SAMPLE = 0\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "STEPS_PER_EPOCH = None\n",
    "\n",
    "# Load processed artifacts\n",
    "print(\"Loading tokenizer and metadata...\")\n",
    "with open(os.path.join(processed_dir, \"tokenizer.pkl\"), \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "with open(os.path.join(processed_dir, \"metadata.json\"), \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "vocab_size = meta['vocab_size']\n",
    "max_length = meta['max_length']\n",
    "print(f\"Vocab size: {vocab_size}, Max length: {max_length}\")\n",
    "\n",
    "# Load cleaned captions into dict: image -> [captions]\n",
    "cleaned_csv = os.path.join(processed_dir, \"cleaned_captions.csv\")\n",
    "df_caps = pd.read_csv(cleaned_csv)\n",
    "descriptions = {}\n",
    "for img, group in df_caps.groupby('image'):\n",
    "    descriptions[img] = group['caption'].tolist()\n",
    "\n",
    "# Load processed_images list if exists; else use keys from descriptions\n",
    "p_images_path = os.path.join(processed_dir, \"processed_images.pkl\")\n",
    "if os.path.exists(p_images_path):\n",
    "    with open(p_images_path, \"rb\") as f:\n",
    "        processed_images = pickle.load(f)\n",
    "else:\n",
    "    processed_images = list(descriptions.keys())\n",
    "\n",
    "if SAMPLE and SAMPLE > 0:\n",
    "    processed_images = processed_images[:SAMPLE]\n",
    "\n",
    "print(f\"Number of images for training: {len(processed_images)}\")\n",
    "\n",
    "# Helper: Data generator\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates batches of (img_features, partial_seq) -> next_word one-hot\"\"\"\n",
    "    def __init__(self, image_list, descriptions, features_dir, tokenizer, max_length, vocab_size, batch_size=64, shuffle=True):\n",
    "        self.image_list = image_list\n",
    "        self.descriptions = descriptions\n",
    "        self.features_dir = features_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.image_list))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_list) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_imgs = [self.image_list[i] for i in batch_idx]\n",
    "        X1, X2, y = self.__data_generation(batch_imgs)\n",
    "        return (X1, X2), y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch_imgs):\n",
    "        X1_list, X2_list, y_list = [], [], []\n",
    "        for img_name in batch_imgs:\n",
    "            feat_path = os.path.join(self.features_dir, img_name + '.npy')\n",
    "            if not os.path.exists(feat_path):\n",
    "                continue\n",
    "            img_feature = np.load(feat_path)\n",
    "            \n",
    "            # ensure flattened 1D vector\n",
    "            img_feature = img_feature.reshape(-1)\n",
    "            caps = self.descriptions.get(img_name, [])\n",
    "            for cap in caps:\n",
    "                seq = self.tokenizer.texts_to_sequences([cap])[0]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq_padded = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
    "                    X1_list.append(img_feature)\n",
    "                    X2_list.append(in_seq_padded)\n",
    "                    \n",
    "                    # one-hot output\n",
    "                    out_vec = np.zeros(self.vocab_size, dtype='uint8')\n",
    "                    if out_seq < self.vocab_size:\n",
    "                        out_vec[out_seq] = 1\n",
    "                    y_list.append(out_vec)\n",
    "        if len(X1_list) == 0:\n",
    "            \n",
    "            # avoid zero-size arrays\n",
    "            return np.zeros((0,2048)), np.zeros((0,self.max_length)), np.zeros((0,self.vocab_size))\n",
    "        X1 = np.vstack(X1_list)\n",
    "        X2 = np.vstack(X2_list)\n",
    "        y = np.vstack(y_list)\n",
    "        return X1, X2, y\n",
    "\n",
    "# Build Model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # Image feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(2048,), name='image_input')\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # Sequence model (decoder)\n",
    "    inputs2 = Input(shape=(max_length,), name='seq_input')\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # Decoder (combine)\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"Defining model...\")\n",
    "model = define_model(vocab_size, max_length)\n",
    "model.summary()\n",
    "\n",
    "#  Prepare Training and Use generator\n",
    "train_gen = DataGenerator(processed_images, descriptions, features_dir, tokenizer, max_length, vocab_size, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Calculate approximate steps per epoch. The generator __len__ provides batches per epoch\n",
    "steps = len(train_gen)\n",
    "print(f\"Steps per epoch (generator length): {steps}\")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = os.path.join(models_dir, 'caption_model_best.h5')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    os.path.join(models_dir, 'caption_model_best.keras'),\n",
    "    save_best_only=True\n",
    ")\n",
    "early = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(train_gen, epochs=EPOCHS, callbacks=[checkpoint, early])\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(models_dir, 'caption_model_final.keras')\n",
    "model.save(final_model_path)\n",
    "print(f\"Model saved to {final_model_path}\")\n",
    "\n",
    "# Helper: generate caption given feature\n",
    "def generate_caption(model, tokenizer, photo_feature, max_length):\n",
    "    in_text = '<start>'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo_feature.reshape(1,-1), sequence], verbose=0)\n",
    "        yhat_index = np.argmax(yhat)\n",
    "        \n",
    "        # map index to word\n",
    "        word = None\n",
    "        for w, idx in tokenizer.word_index.items():\n",
    "            if idx == yhat_index:\n",
    "                word = w\n",
    "                break\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'end' or word == '<end>':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# Evaluate with BLEU (on small subset)\n",
    "def evaluate_model_bleu(model, tokenizer, descriptions, features_dir, test_images, max_length, num_samples=200):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    scores = []\n",
    "    sampled = test_images[:num_samples] if len(test_images) > num_samples else test_images\n",
    "    for img_name in sampled:\n",
    "        feat_path = os.path.join(features_dir, img_name + '.npy')\n",
    "        if not os.path.exists(feat_path):\n",
    "            continue\n",
    "        photo = np.load(feat_path).reshape(-1)\n",
    "        y_pred = generate_caption(model, tokenizer, photo, max_length)\n",
    "        # cleanup predicted: remove <start> and <end>\n",
    "        pred_tokens = [t for t in y_pred.split() if t not in ('<start>', '<end>')]\n",
    "        references = []\n",
    "        # descriptions[img_name] are cleaned captions including <start> and <end>\n",
    "        for ref in descriptions.get(img_name, []):\n",
    "            references.append([w for w in ref.split() if w not in ('<start>', '<end>')])\n",
    "        if not references:\n",
    "            continue\n",
    "        # BLEU-1..4\n",
    "        score = sentence_bleu(references, pred_tokens, smoothing_function=smoothie)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores) if scores else 0.0\n",
    "\n",
    "# Optionally evaluate using the saved best model (if training ended)\n",
    "best_model_path = checkpoint_path if os.path.exists(checkpoint_path) else final_model_path\n",
    "print(\"Loading best model for evaluation:\", best_model_path)\n",
    "best_model_path = os.path.join(models_dir, 'caption_model_best.keras')\n",
    "best_model = load_model(best_model_path)\n",
    "\n",
    "bleu_score = evaluate_model_bleu(best_model, tokenizer, descriptions, features_dir, processed_images, max_length, num_samples=100)\n",
    "print(f\"BLEU (avg) on sample: {bleu_score:.4f}\")\n",
    "\n",
    "# Test on one image\n",
    "test_img = processed_images[0]\n",
    "feat = np.load(os.path.join(features_dir, test_img + '.npy')).reshape(-1)\n",
    "caption = generate_caption(best_model, tokenizer, feat, max_length)\n",
    "print(\"Test image:\", test_img)\n",
    "print(\"Generated caption:\", caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
